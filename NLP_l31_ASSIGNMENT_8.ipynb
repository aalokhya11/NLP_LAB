{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJJHD2zov1YQKidAiNZ+Qm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalokhya11/NLP_LAB/blob/main/NLP_l31_ASSIGNMENT_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT-8\n",
        "\n",
        "Course Name: Natural Language Processing\n",
        "\n",
        "Date: 2/09/2024"
      ],
      "metadata": {
        "id": "XqJ6tR9_N_0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Example text data (you can replace this with any larger corpus) text = \"\"\" Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her. [CO5]\n",
        "\n",
        "(i) Build the Transformer Model on above dataset"
      ],
      "metadata": {
        "id": "ThBYqG-aOLBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cz-Y_DmGMrwE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import string\n",
        "\n",
        "# Preprocess the text data (simple tokenization)\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother...\"\"\"\n",
        "text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "# Simple Dataset for character-level modeling\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.text[idx:idx + self.seq_length]\n",
        "        target = self.text[idx + 1:idx + self.seq_length + 1]\n",
        "        seq = torch.tensor([char_to_idx[c] for c in seq])\n",
        "        target = torch.tensor([char_to_idx[c] for c in target])\n",
        "        return seq, target\n",
        "\n",
        "# Parameters\n",
        "seq_length = 25\n",
        "dataset = TextDataset(text, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Simple Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(self.pos_encoder, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        transformer_output = self.transformer(embedded)\n",
        "        output = self.fc(transformer_output)\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "embed_size = 128\n",
        "num_heads = 4\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) Train the model using 20, 60, 70 epochs"
      ],
      "metadata": {
        "id": "06CgSSQpOQbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch, (seq, target) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(seq)\n",
        "            loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}')\n",
        "\n",
        "# Train the model for 20 epochs\n",
        "train_model(model, 20)\n",
        "# train_model(model, 60)\n",
        "train_model(model, 60)\n",
        "# train_model(model, 70)\n",
        "train_model(model, 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsSIFJeRNQiU",
        "outputId": "c73c0c81-da98-4298-b61c-16bd23a6373e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.4699089129765828\n",
            "Epoch 2/20, Loss: 1.496749758720398\n",
            "Epoch 3/20, Loss: 1.4715338945388794\n",
            "Epoch 4/20, Loss: 1.4810562133789062\n",
            "Epoch 5/20, Loss: 1.4541263182957966\n",
            "Epoch 6/20, Loss: 1.48347802956899\n",
            "Epoch 7/20, Loss: 1.479983965555827\n",
            "Epoch 8/20, Loss: 1.4450130065282185\n",
            "Epoch 9/20, Loss: 1.4387729962666829\n",
            "Epoch 10/20, Loss: 1.4853559335072835\n",
            "Epoch 11/20, Loss: 1.4602420330047607\n",
            "Epoch 12/20, Loss: 1.4423662821451824\n",
            "Epoch 13/20, Loss: 1.4305086533228557\n",
            "Epoch 14/20, Loss: 1.4286735852559407\n",
            "Epoch 15/20, Loss: 1.4580808877944946\n",
            "Epoch 16/20, Loss: 1.4862746000289917\n",
            "Epoch 17/20, Loss: 1.4593278964360554\n",
            "Epoch 18/20, Loss: 1.4344836473464966\n",
            "Epoch 19/20, Loss: 1.464647889137268\n",
            "Epoch 20/20, Loss: 1.456692377726237\n",
            "Epoch 1/60, Loss: 1.4835400183995564\n",
            "Epoch 2/60, Loss: 1.4401435852050781\n",
            "Epoch 3/60, Loss: 1.4280763069788616\n",
            "Epoch 4/60, Loss: 1.5100206136703491\n",
            "Epoch 5/60, Loss: 1.4657132625579834\n",
            "Epoch 6/60, Loss: 1.4608285427093506\n",
            "Epoch 7/60, Loss: 1.4477972984313965\n",
            "Epoch 8/60, Loss: 1.4447897672653198\n",
            "Epoch 9/60, Loss: 1.4457087119420369\n",
            "Epoch 10/60, Loss: 1.4561736583709717\n",
            "Epoch 11/60, Loss: 1.454107403755188\n",
            "Epoch 12/60, Loss: 1.4573275248209636\n",
            "Epoch 13/60, Loss: 1.4477070569992065\n",
            "Epoch 14/60, Loss: 1.4721715847651164\n",
            "Epoch 15/60, Loss: 1.4409071604410808\n",
            "Epoch 16/60, Loss: 1.4598987499872844\n",
            "Epoch 17/60, Loss: 1.4394189516703289\n",
            "Epoch 18/60, Loss: 1.452954649925232\n",
            "Epoch 19/60, Loss: 1.4421941836675007\n",
            "Epoch 20/60, Loss: 1.4373002449671428\n",
            "Epoch 21/60, Loss: 1.4400057395299275\n",
            "Epoch 22/60, Loss: 1.4394823710123699\n",
            "Epoch 23/60, Loss: 1.444727619489034\n",
            "Epoch 24/60, Loss: 1.4258979956309001\n",
            "Epoch 25/60, Loss: 1.4389363129933674\n",
            "Epoch 26/60, Loss: 1.450900634129842\n",
            "Epoch 27/60, Loss: 1.442489782969157\n",
            "Epoch 28/60, Loss: 1.4546724955240886\n",
            "Epoch 29/60, Loss: 1.4142871697743733\n",
            "Epoch 30/60, Loss: 1.4190539518992107\n",
            "Epoch 31/60, Loss: 1.4770047664642334\n",
            "Epoch 32/60, Loss: 1.4457015991210938\n",
            "Epoch 33/60, Loss: 1.4399340152740479\n",
            "Epoch 34/60, Loss: 1.4408469597498577\n",
            "Epoch 35/60, Loss: 1.452626387278239\n",
            "Epoch 36/60, Loss: 1.427372892697652\n",
            "Epoch 37/60, Loss: 1.4612359205881755\n",
            "Epoch 38/60, Loss: 1.4237871964772542\n",
            "Epoch 39/60, Loss: 1.440098722775777\n",
            "Epoch 40/60, Loss: 1.4247522354125977\n",
            "Epoch 41/60, Loss: 1.4371129671732585\n",
            "Epoch 42/60, Loss: 1.4327870607376099\n",
            "Epoch 43/60, Loss: 1.4476082722345989\n",
            "Epoch 44/60, Loss: 1.4455420176188152\n",
            "Epoch 45/60, Loss: 1.4197287559509277\n",
            "Epoch 46/60, Loss: 1.4244170586268108\n",
            "Epoch 47/60, Loss: 1.4618241786956787\n",
            "Epoch 48/60, Loss: 1.4457269112269084\n",
            "Epoch 49/60, Loss: 1.427032748858134\n",
            "Epoch 50/60, Loss: 1.4556135336558025\n",
            "Epoch 51/60, Loss: 1.438611666361491\n",
            "Epoch 52/60, Loss: 1.436501185099284\n",
            "Epoch 53/60, Loss: 1.4081207911173503\n",
            "Epoch 54/60, Loss: 1.4348864555358887\n",
            "Epoch 55/60, Loss: 1.4324649175008137\n",
            "Epoch 56/60, Loss: 1.422912319501241\n",
            "Epoch 57/60, Loss: 1.4290165106455486\n",
            "Epoch 58/60, Loss: 1.405946930249532\n",
            "Epoch 59/60, Loss: 1.4161142905553181\n",
            "Epoch 60/60, Loss: 1.4126265048980713\n",
            "Epoch 1/70, Loss: 1.4558052221934001\n",
            "Epoch 2/70, Loss: 1.4527863264083862\n",
            "Epoch 3/70, Loss: 1.438362677892049\n",
            "Epoch 4/70, Loss: 1.4534996350606282\n",
            "Epoch 5/70, Loss: 1.4312316576639812\n",
            "Epoch 6/70, Loss: 1.434108058611552\n",
            "Epoch 7/70, Loss: 1.4380399783452351\n",
            "Epoch 8/70, Loss: 1.4392487208048503\n",
            "Epoch 9/70, Loss: 1.4282599687576294\n",
            "Epoch 10/70, Loss: 1.4355340003967285\n",
            "Epoch 11/70, Loss: 1.4270426034927368\n",
            "Epoch 12/70, Loss: 1.4308700561523438\n",
            "Epoch 13/70, Loss: 1.4240253766377766\n",
            "Epoch 14/70, Loss: 1.4303517738978069\n",
            "Epoch 15/70, Loss: 1.423791805903117\n",
            "Epoch 16/70, Loss: 1.453607439994812\n",
            "Epoch 17/70, Loss: 1.450057069460551\n",
            "Epoch 18/70, Loss: 1.447101314862569\n",
            "Epoch 19/70, Loss: 1.4227519830067952\n",
            "Epoch 20/70, Loss: 1.4284974336624146\n",
            "Epoch 21/70, Loss: 1.4123882452646892\n",
            "Epoch 22/70, Loss: 1.4209240277608235\n",
            "Epoch 23/70, Loss: 1.4275871117909749\n",
            "Epoch 24/70, Loss: 1.4723464647928874\n",
            "Epoch 25/70, Loss: 1.4207419157028198\n",
            "Epoch 26/70, Loss: 1.4435213009516399\n",
            "Epoch 27/70, Loss: 1.4334239959716797\n",
            "Epoch 28/70, Loss: 1.4157703717549641\n",
            "Epoch 29/70, Loss: 1.430271029472351\n",
            "Epoch 30/70, Loss: 1.4176160097122192\n",
            "Epoch 31/70, Loss: 1.414992094039917\n",
            "Epoch 32/70, Loss: 1.409468372662862\n",
            "Epoch 33/70, Loss: 1.414162039756775\n",
            "Epoch 34/70, Loss: 1.4248487949371338\n",
            "Epoch 35/70, Loss: 1.3946268955866497\n",
            "Epoch 36/70, Loss: 1.4114540815353394\n",
            "Epoch 37/70, Loss: 1.41677991549174\n",
            "Epoch 38/70, Loss: 1.4241122007369995\n",
            "Epoch 39/70, Loss: 1.4403855403264363\n",
            "Epoch 40/70, Loss: 1.4045289357503254\n",
            "Epoch 41/70, Loss: 1.4169466098149617\n",
            "Epoch 42/70, Loss: 1.4243621428807576\n",
            "Epoch 43/70, Loss: 1.4243831634521484\n",
            "Epoch 44/70, Loss: 1.4025077025095622\n",
            "Epoch 45/70, Loss: 1.4301657279332478\n",
            "Epoch 46/70, Loss: 1.4186954895655315\n",
            "Epoch 47/70, Loss: 1.4470357497533162\n",
            "Epoch 48/70, Loss: 1.4367028872172039\n",
            "Epoch 49/70, Loss: 1.4374009370803833\n",
            "Epoch 50/70, Loss: 1.4316095113754272\n",
            "Epoch 51/70, Loss: 1.4107754627863567\n",
            "Epoch 52/70, Loss: 1.4183669090270996\n",
            "Epoch 53/70, Loss: 1.4002739588419597\n",
            "Epoch 54/70, Loss: 1.4210588137308757\n",
            "Epoch 55/70, Loss: 1.4331982135772705\n",
            "Epoch 56/70, Loss: 1.4067086776097615\n",
            "Epoch 57/70, Loss: 1.4323126475016277\n",
            "Epoch 58/70, Loss: 1.410718520482381\n",
            "Epoch 59/70, Loss: 1.4157100915908813\n",
            "Epoch 60/70, Loss: 1.4316397905349731\n",
            "Epoch 61/70, Loss: 1.4548889795939128\n",
            "Epoch 62/70, Loss: 1.4182059367497761\n",
            "Epoch 63/70, Loss: 1.421647071838379\n",
            "Epoch 64/70, Loss: 1.4401908318201702\n",
            "Epoch 65/70, Loss: 1.3988543351491292\n",
            "Epoch 66/70, Loss: 1.4407978057861328\n",
            "Epoch 67/70, Loss: 1.3926646709442139\n",
            "Epoch 68/70, Loss: 1.452258547147115\n",
            "Epoch 69/70, Loss: 1.4045792818069458\n",
            "Epoch 70/70, Loss: 1.433310071627299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " (iii) After training, use the model to generate new text by feeding it an initial seed text\n",
        "\n"
      ],
      "metadata": {
        "id": "bqcGosFuOUTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, seed_text, max_length):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[c] for c in seed_text]).unsqueeze(0)\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "            next_char_idx = torch.argmax(output[:, -1, :], dim=-1).item()\n",
        "            next_char = idx_to_char[next_char_idx]\n",
        "            generated_text += next_char\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using a seed text\n",
        "seed_text = \"once upon a time\"\n",
        "generated = generate_text(model, seed_text, 100)\n",
        "print(\"Generated Text:\\n\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXKuGP5ONbmu",
        "outputId": "8ad372a2-fb08-4c15-f855-7f7c3bf127b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " once upon a time re re re re re re re re re re re re re re re re re re re re re re re re re re re re re re re re re \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iv) Experimenting and Improving the Model by large dataset and hyper tune parameter."
      ],
      "metadata": {
        "id": "6gagnyWQOW_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model parameters for hyperparameter tuning\n",
        "embed_size = 256  # Increase embedding size\n",
        "num_heads = 8     # Increase number of attention heads\n",
        "hidden_dim = 512  # Increase hidden dimension\n",
        "num_layers = 4    # Increase number of layers\n",
        "\n",
        "# Reinitialize the model with new parameters\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers)\n",
        "\n",
        "# Load a larger dataset or continue training on the current dataset\n",
        "# Train the model again with tuned hyperparameters\n",
        "train_model(model, 70)  # Example with 70 epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGK8h97PN3x7",
        "outputId": "4c56c351-b3e7-49cd-b044-74b406f7d633"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70, Loss: 2.845517635345459\n",
            "Epoch 2/70, Loss: 2.05670436223348\n",
            "Epoch 3/70, Loss: 1.6638998985290527\n",
            "Epoch 4/70, Loss: 1.57102104028066\n",
            "Epoch 5/70, Loss: 1.5256163676579793\n",
            "Epoch 6/70, Loss: 1.4968547423680623\n",
            "Epoch 7/70, Loss: 1.4869693915049236\n",
            "Epoch 8/70, Loss: 1.4893366893132527\n",
            "Epoch 9/70, Loss: 1.4566171169281006\n",
            "Epoch 10/70, Loss: 1.477730353673299\n",
            "Epoch 11/70, Loss: 1.459456443786621\n",
            "Epoch 12/70, Loss: 1.4716459115346272\n",
            "Epoch 13/70, Loss: 1.4559752941131592\n",
            "Epoch 14/70, Loss: 1.4556833108266194\n",
            "Epoch 15/70, Loss: 1.4658392270406086\n",
            "Epoch 16/70, Loss: 1.4618924458821614\n",
            "Epoch 17/70, Loss: 1.4506548245747883\n",
            "Epoch 18/70, Loss: 1.4678389231363933\n",
            "Epoch 19/70, Loss: 1.4634709755579631\n",
            "Epoch 20/70, Loss: 1.45652969678243\n",
            "Epoch 21/70, Loss: 1.4480664332707722\n",
            "Epoch 22/70, Loss: 1.4487045208613079\n",
            "Epoch 23/70, Loss: 1.440647006034851\n",
            "Epoch 24/70, Loss: 1.4519389867782593\n",
            "Epoch 25/70, Loss: 1.4688206911087036\n",
            "Epoch 26/70, Loss: 1.484317660331726\n",
            "Epoch 27/70, Loss: 1.4565375248591106\n",
            "Epoch 28/70, Loss: 1.497117241223653\n",
            "Epoch 29/70, Loss: 1.4486980438232422\n",
            "Epoch 30/70, Loss: 1.4673014879226685\n",
            "Epoch 31/70, Loss: 1.4628116687138875\n",
            "Epoch 32/70, Loss: 1.4768085479736328\n",
            "Epoch 33/70, Loss: 1.4604969422022502\n",
            "Epoch 34/70, Loss: 1.4572608470916748\n",
            "Epoch 35/70, Loss: 1.4474848508834839\n",
            "Epoch 36/70, Loss: 1.4378368457158406\n",
            "Epoch 37/70, Loss: 1.4861351251602173\n",
            "Epoch 38/70, Loss: 1.4505247672398884\n",
            "Epoch 39/70, Loss: 1.4643408854802449\n",
            "Epoch 40/70, Loss: 1.471398949623108\n",
            "Epoch 41/70, Loss: 1.4723049799601238\n",
            "Epoch 42/70, Loss: 1.4501119057337444\n",
            "Epoch 43/70, Loss: 1.4245288769404094\n",
            "Epoch 44/70, Loss: 1.4443587462107341\n",
            "Epoch 45/70, Loss: 1.4319593509038289\n",
            "Epoch 46/70, Loss: 1.4401991367340088\n",
            "Epoch 47/70, Loss: 1.4638607501983643\n",
            "Epoch 48/70, Loss: 1.444703737894694\n",
            "Epoch 49/70, Loss: 1.4683225949605305\n",
            "Epoch 50/70, Loss: 1.4461981852849324\n",
            "Epoch 51/70, Loss: 1.4326064586639404\n",
            "Epoch 52/70, Loss: 1.4444528420766194\n",
            "Epoch 53/70, Loss: 1.421434998512268\n",
            "Epoch 54/70, Loss: 1.453704794247945\n",
            "Epoch 55/70, Loss: 1.455885608990987\n",
            "Epoch 56/70, Loss: 1.4545496304829915\n",
            "Epoch 57/70, Loss: 1.4408710797627766\n",
            "Epoch 58/70, Loss: 1.4325405359268188\n",
            "Epoch 59/70, Loss: 1.464539925257365\n",
            "Epoch 60/70, Loss: 1.4391706784566243\n",
            "Epoch 61/70, Loss: 1.437394618988037\n",
            "Epoch 62/70, Loss: 1.4318358103434246\n",
            "Epoch 63/70, Loss: 1.4217724005381267\n",
            "Epoch 64/70, Loss: 1.4136151472727458\n",
            "Epoch 65/70, Loss: 1.499409278233846\n",
            "Epoch 66/70, Loss: 1.465566913286845\n",
            "Epoch 67/70, Loss: 1.4395627578099568\n",
            "Epoch 68/70, Loss: 1.4277767340342205\n",
            "Epoch 69/70, Loss: 1.436423619588216\n",
            "Epoch 70/70, Loss: 1.419015645980835\n"
          ]
        }
      ]
    }
  ]
}