{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm6ZXEcINQurEy1x9BAkp1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalokhya11/NLP_LAB/blob/main/NLP_L31_assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "School of Computer Science Engineering and Artificial Intelligence\n",
        "\n",
        "Course Code: 21CS121\n",
        "\n",
        "Course Name: Natural Language Processing                                                                                  \n",
        "Assignment-06\n",
        "\n",
        "Course Type: Specialization Elective\n",
        "\n",
        "Date: 25/10/2024"
      ],
      "metadata": {
        "id": "4kYLwL3f4thJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Load data fromkeras.datasets and perform following computational analysis:- [CO3]\n",
        "(a) Preprocessing of the Data\n",
        "\n",
        "(b) Divide data into training and testing data set\n",
        "\n",
        "(c) Build the Gated Recurrent Units (GRU) Model\n",
        "\n",
        "(d) Training the GRU Model\n",
        "\n",
        "(e) Text Generation Using the Trained Model\n",
        "\n",
        "(f)  Evaluate Model’s accuracy\n"
      ],
      "metadata": {
        "id": "GwLx0obJ5DrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxoq6EnLyrYV",
        "outputId": "b89535b9-d703-434e-a187-eb7089494d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 285ms/step - accuracy: 0.7005 - loss: 0.5445 - val_accuracy: 0.8250 - val_loss: 0.4048\n",
            "Epoch 2/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 262ms/step - accuracy: 0.8932 - loss: 0.2654 - val_accuracy: 0.8550 - val_loss: 0.3294\n",
            "Epoch 3/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 264ms/step - accuracy: 0.9510 - loss: 0.1427 - val_accuracy: 0.8566 - val_loss: 0.3532\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
            "Generated sentiment for review: [[0.12243753]]\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - accuracy: 0.8477 - loss: 0.3728\n",
            "GRU Model Test Accuracy: 0.8477200269699097\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "\n",
        "# (a) Preprocessing the data\n",
        "max_features = 10000  # number of words to consider\n",
        "maxlen = 100  # cut texts after this number of words\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences (ensure all sequences are of the same length)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# (b) Divide data into training and testing sets\n",
        "# This is already done by IMDB dataset (80/20 split)\n",
        "\n",
        "# (c) Build the GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(GRU(128, return_sequences=True))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# (d) Training the GRU Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# (e) Text Generation (dummy example)\n",
        "# Since the model is trained on binary classification (positive/negative review), actual text generation would need a different task.\n",
        "# Here's a placeholder to indicate text generation (modify accordingly):\n",
        "sample_review = x_test[0].reshape(1, -1)\n",
        "generated_output = model.predict(sample_review)\n",
        "print(f'Generated sentiment for review: {generated_output}')\n",
        "\n",
        "# (f) Evaluate Model's Accuracy\n",
        "gru_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"GRU Model Test Accuracy: {gru_accuracy[1]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Compare accuracy of Long sort term memory and Gated recurrent Unit models for text generation using data from tensorflow.keras.datasets. [CO3]"
      ],
      "metadata": {
        "id": "u-Y22rjR5dMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# (a) Preprocessing the data\n",
        "max_features = 10000  # number of words to consider\n",
        "maxlen = 100  # cut texts after this number of words\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences (ensure all sequences are of the same length)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# (b) Divide data into training and testing sets\n",
        "# This is already done by IMDB dataset (80/20 split)\n",
        "\n",
        "# (c) Build the LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(max_features, 128))\n",
        "model_lstm.add(LSTM(128, return_sequences=True))\n",
        "model_lstm.add(LSTM(128))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# (d) Training the LSTM Model\n",
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_lstm.fit(x_train, y_train, epochs=3, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# (e) Text Generation (dummy example)\n",
        "# Since the model is trained on binary classification (positive/negative review), actual text generation would need a different task.\n",
        "# Here's a placeholder to indicate text generation (modify accordingly):\n",
        "sample_review = x_test[0].reshape(1, -1)\n",
        "generated_output = model_lstm.predict(sample_review)\n",
        "print(f'Generated sentiment for review: {generated_output}')\n",
        "\n",
        "# (f) Evaluate Model's Accuracy\n",
        "lstm_accuracy = model_lstm.evaluate(x_test, y_test)\n",
        "print(f\"LSTM Model Test Accuracy: {lstm_accuracy[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HIwu61x1f9I",
        "outputId": "c1b6d85d-0774-450e-e2be-c1429bd7a38a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 338ms/step - accuracy: 0.7153 - loss: 0.5357 - val_accuracy: 0.8298 - val_loss: 0.3954\n",
            "Epoch 2/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 324ms/step - accuracy: 0.8830 - loss: 0.2958 - val_accuracy: 0.8494 - val_loss: 0.3631\n",
            "Epoch 3/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 336ms/step - accuracy: 0.9230 - loss: 0.2026 - val_accuracy: 0.8416 - val_loss: 0.4030\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
            "Generated sentiment for review: [[0.27734485]]\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 99ms/step - accuracy: 0.8421 - loss: 0.4013\n",
            "LSTM Model Test Accuracy: 0.8422399759292603\n"
          ]
        }
      ]
    }
  ]
}